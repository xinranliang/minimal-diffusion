{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ace494-0ca8-494c-9385-6ef167e6f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/n/fs/xl-diffbia/projects/minimal-diffusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57000bc7-4672-45c5-8677-fa2ff5825d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import copy\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from domain_classifier.cifar_imagenet import DomainClassifier as DC_cifar_imgnet, count_cifar_imgnet\n",
    "from domain_classifier.mnist_flip import DomainClassifier as DC_mnist, count_flip\n",
    "from domain_classifier.fairface import DomainClassifier as DC_fairface, count_fairface, count_fairface_real\n",
    "from domain_classifier.celeba import DomainClassifier as DC_celeba\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dfd98b-d764-45f0-bb60-b3c821199504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArrayToImageLabelShow(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        samples,\n",
    "        labels,\n",
    "        mode, # RGB or L-gray\n",
    "        transform,\n",
    "        transform_show,\n",
    "        target_transform\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # convert from numpy array\n",
    "        self.samples = samples\n",
    "        if labels is not None:\n",
    "            self.labels = labels\n",
    "        else:\n",
    "            self.labels = None\n",
    "        # assert in valid form\n",
    "        if self.samples.min() >= 0 and self.samples.max() <= 1:\n",
    "            self.samples = (self.samples * 255).astype(\"uint8\")\n",
    "        elif self.samples.min() >= -1 and self.samples.max() <= 1:\n",
    "            self.samples = (127.5 * (self.samples + 1)).astype(\"uint8\")\n",
    "        else:\n",
    "            assert self.samples.min() >= 0 and self.samples.max() <= 255\n",
    "        \n",
    "        assert len(self.samples.shape) == 4, \"Images must be a batch\"\n",
    "        if self.labels is not None:\n",
    "            assert self.samples.shape[0] == self.labels.shape[0], \"Number of images and labels must match\"\n",
    "        self.num_items = self.samples.shape[0]\n",
    "        print(f\"Converting {self.num_items} number of samples\")\n",
    "\n",
    "        # transformation\n",
    "        self.transform = transform\n",
    "        self.transform_show = transform_show\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        # 3 channel or 1 channel\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_items\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.samples[index] # height x width x num_channels\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[index]\n",
    "        image = Image.fromarray(np.squeeze(image), mode=self.mode)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image_batch = self.transform(image)\n",
    "        if self.transform_show is not None:\n",
    "            image_show = self.transform_show(image)\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            return image_show, image_batch, label # image, class-condition label\n",
    "        else:\n",
    "            return image_show, image_batch, torch.randint(1, size=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419cd7e-7a83-4933-a634-e282afe1ee64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_image_row(image_batch, nmax=4):\n",
    "    plt.close() # close previous plot\n",
    "    fig, ax = plt.subplots(figsize=(16, 16 * nmax))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(make_grid((image_batch.detach()[:nmax]), nrow=nmax).permute(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80681f94-1f31-4ff0-b45a-fca704861279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change classifier-free guidance value\n",
    "cfg_w = 10.0\n",
    "# change domain ratios\n",
    "female_ratio, male_ratio = 0.5, 0.5\n",
    "# change path to sampled images and labels\n",
    "file_path = os.path.join(\n",
    "            \"./logs/2023-07-25/fairface\", f\"female{female_ratio}_male{male_ratio}\",\n",
    "            \"UNet_diffusionstep_1000_samplestep_250_condition_True_lr_0.0001_bs_128_dropprob_0.1/samples_ema\",\n",
    "            f\"fairface_gendersubset_f{female_ratio}_m{male_ratio}_ema_num50000_guidance{cfg_w}.npz\"\n",
    "            )\n",
    "\n",
    "file_load = np.load(file_path, allow_pickle=True)\n",
    "sampled_images = file_load['arr_0'] # shape = num_samples x height x width x n_channel\n",
    "labels = file_load['arr_1'] # empty if class_cond = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e812f41-5a8c-41ca-b042-e22e55d8c4b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose(\n",
    "                        [\n",
    "                            transforms.Resize(224),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # normalized by imagenet mean std\n",
    "                        ]\n",
    "                    )\n",
    "transform_test_show = transforms.Compose(\n",
    "                        [\n",
    "                            transforms.Resize(224),\n",
    "                            transforms.PILToTensor()\n",
    "                        ]\n",
    "                    )\n",
    "domain_dataset = ArrayToImageLabelShow(\n",
    "                        samples = sampled_images,\n",
    "                        labels = None,\n",
    "                        mode = \"RGB\",\n",
    "                        transform = transform_test,\n",
    "                        transform_show = transform_test_show,\n",
    "                        target_transform = None\n",
    "                    )\n",
    "domain_dataloader = DataLoader(\n",
    "                        domain_dataset,\n",
    "                        batch_size = 10,\n",
    "                        shuffle = True,\n",
    "                        num_workers = 4\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095075e9-7438-41b0-bbc0-359584c0f133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gender classifier\n",
    "classifier = DC_fairface(\n",
    "                num_classes = 2,\n",
    "                arch = \"resnet50\",\n",
    "                pretrained = True,\n",
    "                learning_rate = 0.0001,\n",
    "                weight_decay = 0.00001,\n",
    "                device = \"cuda\"\n",
    "            )\n",
    "classifier.load(\"./logs/2023-07-20/fairface/domain_classifier/bs64_lr0.0001_decay1e-05/ckpt/model_param_final.pth\")\n",
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84746e0-4eec-4f62-ac60-1fa975e93931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_show = 0\n",
    "prob_list = []\n",
    "for image_show, image_batch, _ in iter(domain_dataloader):\n",
    "    # clear_output(wait=True)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # show image\n",
    "        show_image_row(image_show, nmax=10)\n",
    "        \n",
    "        # automatic evaluation\n",
    "        syn_pred = classifier.predict(image_batch)\n",
    "        syn_pred_prob = syn_pred.detach().cpu().numpy() # num_samples x 2\n",
    "        # return predicted probability histogram\n",
    "        prob_list.extend(np.squeeze(syn_pred_prob[:, 0])) # (num_samples, )\n",
    "        # return binary predicted counts\n",
    "        syn_preds = torch.argmax(syn_pred, dim=-1)\n",
    "        syn_preds = syn_preds.detach().cpu().numpy() # (num_samples, )\n",
    "    \n",
    "    # collect human evaluation\n",
    "    human_preds = []\n",
    "    for idx in range(10):\n",
    "        human_value = int(input(\"What gender does this photo show? Enter 0 for Female, 1 for Male, 2 for Unsure.\"))\n",
    "        human_preds.append(human_value)\n",
    "    human_preds = np.array(human_preds, dtype=int)\n",
    "    \n",
    "    print(\"automatic predictions: \", syn_preds)\n",
    "    print(\"human predictions: \", human_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e27b8a-88f4-4e53-8a1f-499673a12230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
